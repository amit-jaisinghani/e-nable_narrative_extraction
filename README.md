# eNable_narrative_extraction

Data analysis for e-nable

Preprocessing.py loads the labelled data in Pandas data frame. <br>
Sequence_Classification.py uses this load data to train model and test its accuracy.

How to run the code: <br>
Preprocessing.py contains path to the labelled data. <br>
This path needs to be updated and run Sequence_Classification.py to execute program.

Steps:
1. Update csvpath with training data path in Preprocessing.py. Example: csvpath = "./data/train.csv" <br>
2. Run Sequence_Classification.py. Example: python3 Sequence_Classification.py

Sample Ouput:
Train on 494 samples, validate on 124 samples
Epoch 1/1

 10/494 [..............................] - ETA: 54s - loss: 3.5091 - acc: 0.7000
 20/494 [>.............................] - ETA: 37s - loss: 3.6013 - acc: 0.6500
 30/494 [>.............................] - ETA: 31s - loss: 3.4198 - acc: 0.6333
 40/494 [=>............................] - ETA: 28s - loss: 3.3807 - acc: 0.6250
 50/494 [==>...........................] - ETA: 26s - loss: 3.3076 - acc: 0.6200
 60/494 [==>...........................] - ETA: 24s - loss: 3.4578 - acc: 0.6500
 70/494 [===>..........................] - ETA: 23s - loss: 3.3926 - acc: 0.6429
 80/494 [===>..........................] - ETA: 22s - loss: 3.4467 - acc: 0.6375
 90/494 [====>.........................] - ETA: 21s - loss: 3.4574 - acc: 0.6556
100/494 [=====>........................] - ETA: 20s - loss: 3.4257 - acc: 0.6300
110/494 [=====>........................] - ETA: 20s - loss: 3.4073 - acc: 0.6364
120/494 [======>.......................] - ETA: 20s - loss: 3.3010 - acc: 0.6083
130/494 [======>.......................] - ETA: 19s - loss: 3.2786 - acc: 0.5923
140/494 [=======>......................] - ETA: 18s - loss: 3.2605 - acc: 0.6071
150/494 [========>.....................] - ETA: 17s - loss: 3.2449 - acc: 0.6000
160/494 [========>.....................] - ETA: 17s - loss: 3.2140 - acc: 0.6000
170/494 [=========>....................] - ETA: 16s - loss: 3.1980 - acc: 0.5824
180/494 [=========>....................] - ETA: 16s - loss: 3.1908 - acc: 0.5944
190/494 [==========>...................] - ETA: 15s - loss: 3.1351 - acc: 0.5842
200/494 [===========>..................] - ETA: 15s - loss: 3.0889 - acc: 0.5750
210/494 [===========>..................] - ETA: 14s - loss: 3.0529 - acc: 0.5714
220/494 [============>.................] - ETA: 14s - loss: 3.0731 - acc: 0.5682
230/494 [============>.................] - ETA: 13s - loss: 3.0620 - acc: 0.5652
240/494 [=============>................] - ETA: 13s - loss: 3.0121 - acc: 0.5667
250/494 [==============>...............] - ETA: 12s - loss: 3.0256 - acc: 0.5640
260/494 [==============>...............] - ETA: 12s - loss: 3.0260 - acc: 0.5654
270/494 [===============>..............] - ETA: 11s - loss: 3.0107 - acc: 0.5556
280/494 [================>.............] - ETA: 10s - loss: 3.0034 - acc: 0.5500
290/494 [================>.............] - ETA: 10s - loss: 2.9580 - acc: 0.5517
300/494 [=================>............] - ETA: 9s - loss: 2.9651 - acc: 0.5567 
310/494 [=================>............] - ETA: 9s - loss: 2.9310 - acc: 0.5613
320/494 [==================>...........] - ETA: 8s - loss: 2.9244 - acc: 0.5594
330/494 [===================>..........] - ETA: 8s - loss: 2.9025 - acc: 0.5545
340/494 [===================>..........] - ETA: 7s - loss: 2.8939 - acc: 0.5559
350/494 [====================>.........] - ETA: 7s - loss: 2.8639 - acc: 0.5543
360/494 [====================>.........] - ETA: 6s - loss: 2.8444 - acc: 0.5583
370/494 [=====================>........] - ETA: 6s - loss: 2.8465 - acc: 0.5568
380/494 [======================>.......] - ETA: 5s - loss: 2.8269 - acc: 0.5579
390/494 [======================>.......] - ETA: 5s - loss: 2.8360 - acc: 0.5590
400/494 [=======================>......] - ETA: 4s - loss: 2.8303 - acc: 0.5525
410/494 [=======================>......] - ETA: 4s - loss: 2.8288 - acc: 0.5561
420/494 [========================>.....] - ETA: 3s - loss: 2.8298 - acc: 0.5524
430/494 [=========================>....] - ETA: 3s - loss: 2.8273 - acc: 0.5512
440/494 [=========================>....] - ETA: 2s - loss: 2.8324 - acc: 0.5500
450/494 [==========================>...] - ETA: 2s - loss: 2.8299 - acc: 0.5489
460/494 [==========================>...] - ETA: 1s - loss: 2.8168 - acc: 0.5500
470/494 [===========================>..] - ETA: 1s - loss: 2.7986 - acc: 0.5468
480/494 [============================>.] - ETA: 0s - loss: 2.8013 - acc: 0.5417
490/494 [============================>.] - ETA: 0s - loss: 2.7819 - acc: 0.5367
494/494 [==============================] - 26s 53ms/step - loss: 2.7804 - acc: 0.5344 - val_loss: 2.3859 - val_acc: 0.7500

Note: 
Sequence_Classification.py splits training set into 80-20 where 80% is used for training and 20% for validation
